{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils.torchvision import *\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "\n",
    "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with $L$ layers have $L$ connections—one between each layer and its subsequent layer, our network has $\\frac{L(L+1)}{2}$ direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. [Paper](https://arxiv.org/pdf/1608.06993)\n",
    "\n",
    "<center>\n",
    "<img width=\"800\" src=\"https://i.ibb.co/7GsdcsZ/image-2024-06-13-141410503.png\" alt=\"image-2024-06-13-141410503\" border=\"0\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction(nn.Module):\n",
    "  def __init__(self, num_init_channels):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(3, num_init_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.norm = nn.BatchNorm2d(num_init_channels)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.pool(self.relu(self.norm(self.conv(x))))\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "  def __init__(self, input_channels, growth_rate, bn_size, drop_rate):\n",
    "    super().__init__()\n",
    "    fix_channel = bn_size * growth_rate\n",
    "    self.norm1 = nn.BatchNorm2d(input_channels)\n",
    "    self.relu1 = nn.ReLU(inplace=True)\n",
    "    self.conv1 = nn.Conv2d(input_channels, fix_channel, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "    self.norm2 = nn.BatchNorm2d(fix_channel)\n",
    "    self.relu2 = nn.ReLU(inplace=True)\n",
    "    self.conv2 = nn.Conv2d(fix_channel, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    self.drop_rate = float(drop_rate)\n",
    "\n",
    "  def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "  def forward(self, input):\n",
    "      if isinstance(input, Tensor): prev_features = [input]\n",
    "      else: prev_features = input\n",
    "      bottleneck_output = self.bn_function(prev_features)\n",
    "      new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "      return F.dropout(new_features, p=self.drop_rate)\n",
    "\n",
    "class DenseBlock(nn.ModuleDict):\n",
    "  def __init__(self, num_layers, input_channels, bn_size, growth_rate, drop_rate=0.1):\n",
    "    super().__init__()\n",
    "    for i in range(num_layers):\n",
    "      layer = DenseLayer(input_channels + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "      self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
    "\n",
    "  def forward(self, init_features):\n",
    "    features = [init_features]\n",
    "    for name, layer in self.items():\n",
    "        new_features = layer(features)\n",
    "        features.append(new_features)\n",
    "    return torch.cat(features, 1)\n",
    "\n",
    "class Transition(nn.Sequential):\n",
    "  def __init__(self, input_features, output_features):\n",
    "    super().__init__()\n",
    "    self.norm = nn.BatchNorm2d(input_features)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    self.conv = nn.Conv2d(input_features, output_features, kernel_size=1, stride=1, bias=False)\n",
    "    self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.pool(self.conv(self.relu(self.norm(x))))\n",
    "\n",
    "class SimplifiedDenseNet(nn.Module):\n",
    "  def __init__(self, num_classes, growth_rate=32, num_init_channels=64,\n",
    "               layer_each_block=(6, 12, 24, 16), bn_size=4, drop_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    # Feature extraction\n",
    "    self.feature_extraction = FeatureExtraction(num_init_channels)\n",
    "\n",
    "    # DenseBlocks\n",
    "    num_channels = num_init_channels\n",
    "    self.denseblock1 = DenseBlock(layer_each_block[0], num_channels, bn_size, growth_rate)\n",
    "    concated_channels = num_channels + layer_each_block[0] * growth_rate\n",
    "    self.transition1 = Transition(concated_channels, concated_channels//2)\n",
    "\n",
    "    num_channels *= 2\n",
    "    self.denseblock2 = DenseBlock(layer_each_block[1], num_channels, bn_size, growth_rate)\n",
    "    concated_channels = num_channels + layer_each_block[1] * growth_rate\n",
    "    self.transition2 = Transition(concated_channels, concated_channels//2)\n",
    "\n",
    "    num_channels *= 2\n",
    "    self.denseblock3 = DenseBlock(layer_each_block[2], num_channels, bn_size, growth_rate)\n",
    "    concated_channels = num_channels + layer_each_block[2] * growth_rate\n",
    "    self.transition3 = Transition(concated_channels, concated_channels//2)\n",
    "\n",
    "    num_channels *= 2\n",
    "    self.denseblock4 = DenseBlock(layer_each_block[3], num_channels, bn_size, growth_rate)\n",
    "\n",
    "    # Output\n",
    "    self.final_norm = nn.BatchNorm2d(num_channels*2)\n",
    "    self.final_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "    self.classifier = nn.Linear(num_channels*2, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.feature_extraction(x)\n",
    "\n",
    "    x = self.denseblock1(x)\n",
    "    x = self.transition1(x)\n",
    "\n",
    "    x = self.denseblock2(x)\n",
    "    x = self.transition2(x)\n",
    "\n",
    "    x = self.denseblock3(x)\n",
    "    x = self.transition3(x)\n",
    "\n",
    "    x = self.denseblock4(x)\n",
    "    x = self.final_pool(self.final_norm(x))\n",
    "\n",
    "    x = torch.flatten(x, 1)\n",
    "    return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(12, 3, 224, 224)\n",
    "model = SimplifiedDenseNet(num_classes=1000)\n",
    "y = model(x)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DenseNet architecture is highly computationally efficient as a result of\n",
    "feature reuse. However, a naïve DenseNet implementation can require a significant amount of GPU memory: If not properly managed, pre-activation batch normalization and contiguous convolution operations can produce feature maps that grow quadratically with network depth. This implementation follows the strategy of shared memory allocations to reduce the memory cost for storing feature maps from quadratic to linear.\n",
    "\n",
    "<center>\n",
    "<img width=\"800\" src=\"https://i.ibb.co/PjxygZD/image.png\" alt=\"image\" border=\"0\">\n",
    "<img width=\"800\" src=\"https://i.ibb.co/G207C92/image.png\" alt=\"image\" border=\"0\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tench', 'goldfish', 'great white shark', 'tiger shark', 'hammerhead', 'electric ray', 'stingray', 'cock', 'hen', 'ostrich']\n"
     ]
    }
   ],
   "source": [
    "with open(\"/workspace/dataset/vision-reg/imagenet-1k/labels.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    _IMAGENET_CATEGORIES = [line.strip().strip('\"')[:-2] for line in lines]\n",
    "    \n",
    "print(_IMAGENET_CATEGORIES[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchvision version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features: int, growth_rate: int, bn_size: int,\n",
    "                 drop_rate: float, memory_efficient: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.drop_rate = float(drop_rate)\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
    "        return bottleneck_output\n",
    "    \n",
    "    def any_requires_grad(self, input: List[Tensor]) -> bool:\n",
    "        for tensor in input:\n",
    "            if tensor.requires_grad:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def call_checkpoint_bottleneck(self, input: List[Tensor]) -> Tensor:\n",
    "        def closure(*inputs):\n",
    "            return self.bn_function(inputs)\n",
    "\n",
    "        return cp.checkpoint(closure, *input, use_reentrant=False)\n",
    "\n",
    "    @torch.jit._overload_method\n",
    "    def forward(self, input: List[Tensor]) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @torch.jit._overload_method\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor: \n",
    "        if isinstance(input, Tensor):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "\n",
    "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
    "            if torch.jit.is_scripting():\n",
    "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
    "\n",
    "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
    "        else:\n",
    "            bottleneck_output = self.bn_function(prev_features)\n",
    "\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.ModuleDict):\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        num_input_features: int,\n",
    "        bn_size: int,\n",
    "        growth_rate: int,\n",
    "        drop_rate: float,\n",
    "        memory_efficient: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features: Tensor) -> Tensor:\n",
    "        print(\"New Block ===\")\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            print()\n",
    "            print(f\"Feature: {new_features.shape}\")\n",
    "            print()\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features: int, num_output_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.norm = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Densely Connected Convolutional Networks\n",
    "\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "            (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "        but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        growth_rate: int = 32,\n",
    "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n",
    "        num_init_features: int = 64,\n",
    "        bn_size: int = 4,\n",
    "        drop_rate: float = 0,\n",
    "        num_classes: int = 1000,\n",
    "        memory_efficient: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "                    (\"norm0\", nn.BatchNorm2d(num_init_features)),\n",
    "                    (\"relu0\", nn.ReLU(inplace=True)),\n",
    "                    (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module(\"transition%d\" % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchvision User Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 classes: ['goldfish', 'tench', 'axolotl', 'anemone fish', 'coho']\n",
      "Probabilities: tensor([1.0000e+00, 2.1803e-06, 9.4948e-07, 1.8117e-07, 1.7783e-07])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained DenseNet121 model\n",
    "model = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\", progress=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the image preprocessing steps\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load an image\n",
    "img_path = '/workspace/dataset/samples/fish.jpeg'\n",
    "img = Image.open(img_path)\n",
    "img_t = preprocess(img)\n",
    "input_tensor = img_t.unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "# Print the top 5 most probable classes\n",
    "_, indices = torch.topk(probabilities, 5)\n",
    "print(f'Top 5 classes: {np.array(_IMAGENET_CATEGORIES)[indices].tolist()}')\n",
    "print(f'Probabilities: {probabilities[indices]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
