{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["RzsYBhcaLfHv","QhDM7TU-qecq","SLfTTwUaVTSn","gsWLCpBnqnSI","K6hOKdxSwiXi","o19295FjYN6Z","8IkziRNIf1ID","hRge2SX1YSQK","Vmo19ThCZ0vi","cI7Xlyp5aoAq","eFsNZRZXbrgK","0k9diOeMdmwJ","uhkGM1zzd9ty","rv-rYytdeWmi","OLGpes5Klfcx","DvWN1xhBldby","OYLSQFXlqXwp","LGAep9BjpkCV"],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyO7SibCNtI7qvpDreO6hsZJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Model Acceleration\n","\n","Ngày nay bên cạnh nghiên cứu ra các mô hình học sâu chính xác hơn, nhanh hơn thì việc ứng dụng đưa các mô hình học sâu vào trong các sẩn phẩm cũng không kém phần quan trọng và gặp rất nhiều thách thức. Đặc biệt trong việc chuyển từ mô hình được viết bằng framework này sang framework khác vì mỗi thư viện có các hàm và kiểu dữ liệu khác nhau. Sau khi huấn luyện mô hình, chúng ta cần chuyển đổi mô hình sao cho tương thích với hardware sử dụng (e.g, Intel CPU, Nvidia GPU, ARM CPU, etc). Khi nghiên cứu thử nghiệm mô hình mình thường sử dụng pytorch vì dễ sử dụng và cộng đồng nghiên cứu cũng dùng torch nhiều rất tiện việc tra cứu. Tuy nhiên, khi triển khai thành sản phẩm thì trong một số công cụ lại chỉ hỗ trợ tensorflow do đó để sử dụng cần phải chuyển mô hình từ torch sang tensorflow. Lúc này chúng ta cần một dạng dữ liệu chuẩn cho các hàm cũng như các dạng dữ liệu (data types) để chuyển đổi. Và ONNX là chìa khóa có thể giải quyết tất cả vấn đề trên."],"metadata":{"id":"bS2auNYxRJYi"}},{"cell_type":"markdown","source":["## Tăng tốc độ inference với Torch.compile\n","`torch.compile` làm cho Pytorch code chạy nhanh hơn bằng việc sử dụng JIT-compiling để convert code trở thành optimized kernels nhưng lại ít yêu cầu việc thay đổi code nên việc sử dụng tương đối đơn giản nhất."],"metadata":{"id":"RzsYBhcaLfHv"}},{"cell_type":"markdown","source":["### Thử nghiệm chạy 10 iteration with densnet121"],"metadata":{"id":"QhDM7TU-qecq"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","from torchvision import transforms\n","import torch._dynamo\n","from PIL import Image\n","\n","print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4J2FO14_UnTx","executionInfo":{"status":"ok","timestamp":1718319549250,"user_tz":-420,"elapsed":6318,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"fcbdbe78-9073-4864-a128-e183a0120c55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["def init_model():\n","    return torchvision.models.densenet121().to(torch.float32).cuda()\n","\n","def generate_data(b):\n","    return (\n","        torch.randn(b, 3, 128, 128).to(torch.float32).cuda(),\n","        torch.randint(1000, (b,)).cuda(),\n","    )\n","\n","def timed(fn):\n","    start = torch.cuda.Event(enable_timing=True)\n","    end = torch.cuda.Event(enable_timing=True)\n","    start.record()\n","    result = fn()\n","    end.record()\n","    torch.cuda.synchronize()\n","    return result, start.elapsed_time(end) / 1000\n","\n","N_ITERS = 10\n","model = init_model()\n","torch._dynamo.reset()\n","model_opt = torch.compile(model, mode=\"reduce-overhead\")\n","\n","eager_times = []\n","for i in range(N_ITERS):\n","    inp = generate_data(16)[0]\n","    with torch.no_grad():\n","        _, eager_time = timed(lambda: model(inp))\n","    eager_times.append(eager_time)\n","    print(f\"eager eval time {i}: {eager_time}\")\n","\n","print(\"~\" * 10)\n","compile_times = []\n","for i in range(N_ITERS):\n","    inp = generate_data(16)[0]\n","    with torch.no_grad():\n","        _, compile_time = timed(lambda: model_opt(inp))\n","    compile_times.append(compile_time)\n","    print(f\"compile eval time {i}: {compile_time}\")\n","print(\"~\" * 10)\n","\n","eager_med = np.median(eager_times)\n","compile_med = np.median(compile_times)\n","speedup = eager_med / compile_med\n","assert(speedup > 1)\n","print(f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\n","print(\"~\" * 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdWtaSrOT95t","executionInfo":{"status":"ok","timestamp":1718312670142,"user_tz":-420,"elapsed":190287,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"c869bce2-666f-4f28-e0c4-611ad7a94f2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["eager eval time 0: 1.036719970703125\n","eager eval time 1: 0.02999740791320801\n","eager eval time 2: 0.02996428871154785\n","eager eval time 3: 0.029912832260131837\n","eager eval time 4: 0.02993699264526367\n","eager eval time 5: 0.029917695999145507\n","eager eval time 6: 0.02991971206665039\n","eager eval time 7: 0.020753696441650392\n","eager eval time 8: 0.01882111930847168\n","eager eval time 9: 0.018932416915893556\n","~~~~~~~~~~\n","compile eval time 0: 187.395515625\n","compile eval time 1: 0.7736571655273438\n","compile eval time 2: 0.016364959716796874\n","compile eval time 3: 0.016267263412475585\n","compile eval time 4: 0.01669526481628418\n","compile eval time 5: 0.018122495651245116\n","compile eval time 6: 0.016326751708984375\n","compile eval time 7: 0.0165479679107666\n","compile eval time 8: 0.016340991973876954\n","compile eval time 9: 0.016363519668579102\n","~~~~~~~~~~\n","(eval) eager median: 0.02991870403289795, compile median: 0.016456463813781737, speedup: 1.818051822764137x\n","~~~~~~~~~~\n"]}]},{"cell_type":"markdown","source":["### Thử nghiệm với mô hình Pytorch ResNet50 cho classification."],"metadata":{"id":"SLfTTwUaVTSn"}},{"cell_type":"code","source":["print(f\"Non experimental in-tree backends: {torch._dynamo.list_backends()}\")\n","print(f\"Experimental or debug in-tree backends: {torch._dynamo.list_backends(None)}\")\n","print(f\"Mode in torch.compile: {torch._inductor.list_mode_options()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2WS596YW2bo","executionInfo":{"status":"ok","timestamp":1718312003856,"user_tz":-420,"elapsed":529,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"55f234a3-6463-481e-c71c-d5749593a521"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Non experimental in-tree backends: ['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'openxla_eval', 'tvm']\n","Experimental or debug in-tree backends: ['aot_eager', 'aot_eager_decomp_partition', 'aot_eager_default_partitioner', 'aot_torchxla_trace_once', 'aot_torchxla_trivial', 'aot_ts', 'cudagraphs', 'dynamo_accuracy_minifier_backend', 'dynamo_minifier_backend', 'eager', 'eager_debug', 'inductor', 'non_leaf_compile_error_TESTING_ONLY', 'onnxrt', 'openxla', 'openxla_eval', 'pre_dispatch_eager', 'relu_accuracy_error_TESTING_ONLY', 'relu_compile_error_TESTING_ONLY', 'relu_runtime_error_TESTING_ONLY', 'torchxla_trace_once', 'torchxla_trivial', 'ts', 'tvm']\n","Mode in torch.compile: {'default': {}, 'reduce-overhead': {'triton.cudagraphs': True}, 'max-autotune-no-cudagraphs': {'max_autotune': True}, 'max-autotune': {'max_autotune': True, 'triton.cudagraphs': True}}\n"]}]},{"cell_type":"markdown","source":["Determining the \"best\" backend depends on the specific use case and requirements. Here are some points to consider:\n","\n","- **Performance**: inductor and cudagraphs are typically good choices for high performance on supported hardware.\n","- **Compatibility**: onnxrt (ONNX Runtime) and tvm can be good for deploying models across various platforms and devices.\n","- **Stability**: Stick to the non-experimental backends like inductor, onnxrt, openxla, and tvm for stable and production-ready applications.\n","- **Specific Needs**: Experimental backends may provide features that are not yet available in stable backends, useful for development and research purposes.\n","\n","The mode attribute in `torch.compile(`) specifies different optimization strategies for compiling PyTorch models:\n","- **default**: Balances performance and overhead.\n","- **reduce-overhead**: Minimizes Python overhead using CUDA graphs, useful for small batches. May increase memory usage due to workspace memory caching. Works only for CUDA graphs that don’t mutate inputs. Use `TORCH_LOG=perf_hints` for debugging.\n","- **max-autotune**: Uses Triton-based matrix multiplications and convolutions with CUDA graphs enabled by default.\n","- **max-autotune-no-cudagraphs**: Similar to \"max-autotune\" but without CUDA graphs."],"metadata":{"id":"rJMeSC4WXi55"}},{"cell_type":"code","source":["model = torchvision.models.resnet50(weights=\"ResNet50_Weights.IMAGENET1K_V1\", progress=True)\n","compiled_model = torch.compile(model, backend=\"eager\")\n","model.eval()\n","compiled_model.eval()"],"metadata":{"id":"r757F4_ZZ_k4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the image preprocessing steps\n","preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load an image\n","img_path = '/content/cock.png'\n","img = Image.open(img_path)\n","img_t = preprocess(img)\n","input_tensor = img_t.unsqueeze(0)\n","\n","# Perform inference\n","with torch.inference_mode():\n","    output = model(input_tensor)\n","    output_ = compiled_model(input_tensor)\n","    print(torch.argmax(output, dim=1), torch.argmax(output_, dim=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUUVF7pAaBU4","executionInfo":{"status":"ok","timestamp":1718312921512,"user_tz":-420,"elapsed":1155,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"36cb6f18-6234-4cf9-dfa1-da8658cb8f0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([7]) tensor([7])\n"]}]},{"cell_type":"code","source":["%%time\n","output = model(input_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2VtMPTZWbnvJ","executionInfo":{"status":"ok","timestamp":1718312991694,"user_tz":-420,"elapsed":997,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"2fa90642-2f7e-4b3e-bad1-853740736993"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 237 ms, sys: 51.9 ms, total: 289 ms\n","Wall time: 430 ms\n"]}]},{"cell_type":"code","source":["%%time\n","with torch.inference_mode():\n","  output = compiled_model(input_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRdbdRjIb0a3","executionInfo":{"status":"ok","timestamp":1718312998901,"user_tz":-420,"elapsed":392,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"b3c9609a-a39e-4f8a-f9ea-fcbd9b5db198"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 161 ms, sys: 2.9 ms, total: 163 ms\n","Wall time: 165 ms\n"]}]},{"cell_type":"markdown","source":["## Tăng tốc độ với Torch Scripting\n","\n","Khi mô hình gặp lỗi khi chuyển sang format ONNX, ta có thể sử dụng Torch Script. Đây là một phương pháp để chuyển mô hình Pytorch về dạng serializable và optmizable. Chỉ khi ở dạng serialized, script này có thể chạy không cần môi trường python, dependencies phức tạp. Ví dụ có thể inference [TorchScript models ở C++](https://pytorch.org/tutorials/advanced/cpp_export.html)."],"metadata":{"id":"gsWLCpBnqnSI"}},{"cell_type":"markdown","source":["### Tracing and Scripting"],"metadata":{"id":"jNvuFqsEwd87"}},{"cell_type":"markdown","source":["### SwinTransformer-B Conversion\n","Ta sẽ thực hiện chuyển đổi SwinTransformer-B dưới dạng Torch Script."],"metadata":{"id":"K6hOKdxSwiXi"}},{"cell_type":"code","source":["model = torchvision.models.swin_b(weights=\"Swin_B_Weights.IMAGENET1K_V1\", progress=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFrPfUxovRZK","executionInfo":{"status":"ok","timestamp":1718318320970,"user_tz":-420,"elapsed":8843,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"143c38e7-8ab0-441b-a92c-aa1abef121f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/swin_b-68c6b09e.pth\" to /root/.cache/torch/hub/checkpoints/swin_b-68c6b09e.pth\n","100%|██████████| 335M/335M [00:06<00:00, 52.6MB/s]\n"]}]},{"cell_type":"code","source":["torch.save(model, \"swin-b.pt\")"],"metadata":{"id":"nCxkseCwwm3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = torch.load('swin-b.pt')\n","scripted_model = torch.jit.script(model)\n","print(scripted_model)\n","scripted_model.save('scripted-swin-b.pt')"],"metadata":{"id":"fEDQ_UfaqzxH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Benchmark accuracy: Xem [ImageNet1k class list](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/) để check độ chính xác"],"metadata":{"id":"KUuHvKKlw1Gk"}},{"cell_type":"code","source":["def preprocess(path):\n","  augment = transforms.Compose([\n","      transforms.Resize(224),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","  ])\n","\n","  image = Image.open(path)\n","  image = augment(image)\n","  return image.unsqueeze(0)\n","\n","inputs = preprocess(\"/content/junco.png\")\n","\n","model.eval()\n","scripted_model.eval()\n","\n","output = model(inputs)\n","output_ = scripted_model(inputs)\n","print(torch.argmax(output, dim=1), torch.argmax(output_, dim=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZB9Kev8w3ay","executionInfo":{"status":"ok","timestamp":1718319215287,"user_tz":-420,"elapsed":3034,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"1dcc93bb-475d-4bc9-bd14-d220a897c61e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([13]) tensor([13])\n"]}]},{"cell_type":"markdown","source":["Benchmark Time"],"metadata":{"id":"rqxtMEX3zv-r"}},{"cell_type":"code","source":["inputs = [torch.randn(24, 3, 224, 224),\n","          torch.randn(24, 3, 224, 224),\n","          torch.randn(24, 3, 224, 224),\n","          torch.randn(24, 3, 224, 224)]"],"metadata":{"id":"WcY2qLw50Fkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%timeit\n","with torch.inference_mode():\n","  for input in inputs:\n","    output = model(input)"],"metadata":{"id":"SzYfIGxBxk7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%timeit\n","for input in inputs:\n","  output = scripted_model(inputs)"],"metadata":{"id":"tNZorP0Hz6Hb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Open Neural Network Exchange (ONNX)\n","\n","**Giới thiệu về ONNX:** ONNX được xem là một ngôn ngữ đại diện cho công cụ toán học và thường được biểu diễn dưới dạng đồ thị (graph). Với ONNX, ta có thể xây dựng một quy trình deploy model độc lập hoàn toàn với các framework huấn luyện model (e.g, TensorFlow, Pytorch, etc).\n","\n","**Cách hoạt động của ONNX**:\n","- Xây dụng một đồ thị ONNX nghĩa là sử dụng toán tử hay ngôn ngữ riêng của ONNX. Một đồ thị sẽ có những đỉnh (nodes), ta hình dung nodes trong đồ thị bằng phép toán đơn giản.\n","\n","\\begin{gather*}\n","y = x \\times a  + c \\\\\n","y = r + c\n","\\end{gather*}\n","\n","- Ta có các nodes $x$, node $c$, node $r$ là kết quả trung gian của phép tính trên, và node $y$ là kết quả. Và các phép tính $\\times$ và $+$ sẽ là các operator trên đường nối các nodes.\n","- Ta đều biết, để chạy mô hình, ta cần có môi trường cài đặt những dependencies cần thiết. Nhưng với ONNX, ta chỉ cần một runtime có sẵn bởi ONNX để chạy cái graph là một dãy những phép tính toán học trên những con số. Và runtime này có thể được code ở bất kỳ ngôn ngữ nào (e.g, C, java, python, javascript, C#, etc) để thực hiện inference với mô hình.\n","- Một điều lưu ý khi sử dụng ONNX là hạn chế sử dụng các câu lệnh `if/else/loop` vì ONNX thực chất chỉ là một cấu trúc cây (graph) thực hiện các phép tính toán trên tensors (ma trận/vector) nên sử dụng các câu lệnh trên sẽ làm cấu trúc cây trở nên chồng chéo phức tạp."],"metadata":{"id":"o19295FjYN6Z"}},{"cell_type":"markdown","source":["### Basic Example"],"metadata":{"id":"8IkziRNIf1ID"}},{"cell_type":"code","source":["!pip install onnx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmSip1cqdwJq","executionInfo":{"status":"ok","timestamp":1718313511266,"user_tz":-420,"elapsed":13227,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"1976b3b9-ebd0-4bf7-e262-bbd177ee0c8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.16.1\n"]}]},{"cell_type":"markdown","source":["Chúng ta thử tạo một sơ đồ (graph) thể hiện phép tính sau: $Y = XA + B$. Đầu tiên, ta cần import các hàm cần thiết"],"metadata":{"id":"Eg6FP6v2eTUB"}},{"cell_type":"code","source":["from onnx import TensorProto\n","from onnx.helper import (\n","    make_tensor_value_info,         # khởi tạo biến với shape và kiểu dữ liệu\n","    make_node,                      # Tạo một node đại diện cho phép tính\n","    make_graph,                     # Tạo sơ đồ tính toán dựa vào biến và phép tính trên\n","    make_model                      # Hàm cuối cùng để nhúng vào thêm metadata\n",")\n","from onnx.checker import check_model"],"metadata":{"id":"gmP0zH_XRNlC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bước 1, ta cần khởi tạo các biến Tensor cần thiết với shape undefined (None):"],"metadata":{"id":"hjPZ5CyheGuR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyr9QJIYQ6oO"},"outputs":[],"source":["X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n","A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n","B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n","Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])"]},{"cell_type":"markdown","source":["Bước 2, ta tạo các node đại diện cho phép tính sau khi đã có được các biến:"],"metadata":{"id":"D8Srq4eceJyS"}},{"cell_type":"code","source":["node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n","node2 = make_node('Add', ['XA', 'B'], ['Y'])"],"metadata":{"id":"6TaDBXTtTPyD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ở `node1`, `X` và `A` là input, `XA` là output. Tương tự ở `node2`, `XA` và `B` là input còn `Y` là output.\n","\n","Bước 3, ta tạo sơ đô tính toán (graph) với các biến và phép tính đã được khởi tạo sẵn ở trên để tổng quát hóa quá trình định nghĩa `input` và `output` cuối cùng là gì:"],"metadata":{"id":"AvW5fyjxevw8"}},{"cell_type":"code","source":["graph = make_graph([node1, node2],  # nodes\n","                    'lr',           # a name\n","                    [X, A, B],      # inputs\n","                    [Y])            # outputs"],"metadata":{"id":"c_tg11GaTS5S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Khởi tạo model từ graph trên:"],"metadata":{"id":"tqWyyZ1UfbDq"}},{"cell_type":"code","source":["onnx_model = make_model(graph)\n","check_model(onnx_model)"],"metadata":{"id":"lcvUuYxvWA1b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ta có thể in ra các attribute có trong biến model onnx_model này bằng cách:"],"metadata":{"id":"B9vCfyULfewe"}},{"cell_type":"code","source":["print(onnx_model.graph.input)\n","print(onnx_model.graph.output)\n","print(onnx_model.graph.node)"],"metadata":{"id":"I9k-CcuMWX0R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Một cách in khác cho đễ nhìn"],"metadata":{"id":"KHwXm2fVfsT0"}},{"cell_type":"code","source":["def shape2tuple(shape):\n","    return tuple(getattr(d, 'dim_value', 0) for d in shape.dim)\n","\n","print('** inputs **')\n","for obj in onnx_model.graph.input:\n","    print(\"name=%r dtype=%r shape=%r\" % (\n","        obj.name, obj.type.tensor_type.elem_type,\n","        shape2tuple(obj.type.tensor_type.shape)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_7Wz5jS2WcDZ","executionInfo":{"status":"ok","timestamp":1718313987562,"user_tz":-420,"elapsed":378,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"df7b2bbd-1e6e-46e4-b80f-f30487e2744f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["** inputs **\n","name='X' dtype=1 shape=(0, 0)\n","name='A' dtype=1 shape=(0, 0)\n","name='B' dtype=1 shape=(0, 0)\n"]}]},{"cell_type":"code","source":["print(onnx_model.graph.node)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tshx8U5gWtay","executionInfo":{"status":"ok","timestamp":1718314010174,"user_tz":-420,"elapsed":392,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"6ce9a85b-a309-4886-d97b-0ed17c604c5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[input: \"X\"\n","input: \"A\"\n","output: \"XA\"\n","op_type: \"MatMul\"\n",", input: \"XA\"\n","input: \"B\"\n","output: \"Y\"\n","op_type: \"Add\"\n","]\n"]}]},{"cell_type":"markdown","source":["### Data Serialization"],"metadata":{"id":"hRge2SX1YSQK"}},{"cell_type":"code","source":["import numpy\n","from onnx.numpy_helper import from_array, to_array\n","from onnx import TensorProto\n","\n","numpy_tensor = numpy.array([0, 1, 4, 5, 3], dtype=numpy.float32)\n","print(type(numpy_tensor))\n","\n","onnx_tensor = from_array(numpy_tensor)\n","print(type(onnx_tensor))\n","\n","serialized_tensor = onnx_tensor.SerializeToString()\n","print(type(serialized_tensor))\n","\n","with open(\"saved_tensor.pb\", \"wb\") as f:\n","    f.write(serialized_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShGKYtMgYM7i","executionInfo":{"status":"ok","timestamp":1718314098373,"user_tz":-420,"elapsed":454,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"0f904edd-3623-48cd-83bd-ea250e54c61a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n","<class 'onnx.onnx_ml_pb2.TensorProto'>\n","<class 'bytes'>\n"]}]},{"cell_type":"code","source":["with open(\"saved_tensor.pb\", \"rb\") as f:\n","    serialized_tensor = f.read()\n","print(type(serialized_tensor))\n","\n","onnx_tensor = TensorProto()\n","onnx_tensor.ParseFromString(serialized_tensor)\n","print(type(onnx_tensor))\n","\n","numpy_tensor = to_array(onnx_tensor)\n","print(numpy_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFVwHEb7YpbZ","executionInfo":{"status":"ok","timestamp":1718314099897,"user_tz":-420,"elapsed":3,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"210fcefb-dfe6-4a06-897d-e57a166f936f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'bytes'>\n","<class 'onnx.onnx_ml_pb2.TensorProto'>\n","[0. 1. 4. 5. 3.]\n"]}]},{"cell_type":"code","source":["import onnx\n","import pprint\n","pprint.pprint([p for p in dir(onnx)\n","               if p.endswith('Proto') and p[0] != '_'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttw1q0fhYo7x","executionInfo":{"status":"ok","timestamp":1718314110810,"user_tz":-420,"elapsed":423,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"73a35e5f-0e09-42db-c8c2-434536234279"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['AttributeProto',\n"," 'FunctionProto',\n"," 'GraphProto',\n"," 'MapProto',\n"," 'ModelProto',\n"," 'NodeProto',\n"," 'OperatorProto',\n"," 'OperatorSetIdProto',\n"," 'OperatorSetProto',\n"," 'OptionalProto',\n"," 'SequenceProto',\n"," 'SparseTensorProto',\n"," 'StringStringEntryProto',\n"," 'TensorProto',\n"," 'TensorShapeProto',\n"," 'TrainingInfoProto',\n"," 'TypeProto',\n"," 'ValueInfoProto']\n"]}]},{"cell_type":"code","source":["from onnx import load_tensor_from_string\n","\n","with open(\"saved_tensor.pb\", \"rb\") as f:\n","    serialized = f.read()\n","proto = load_tensor_from_string(serialized)\n","print(type(proto))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpEiiTuzZB_q","executionInfo":{"status":"ok","timestamp":1718314116670,"user_tz":-420,"elapsed":381,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"d6b2c6b4-0766-4d47-9de9-f149665175a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'onnx.onnx_ml_pb2.TensorProto'>\n"]}]},{"cell_type":"markdown","source":["### Initializer, default value"],"metadata":{"id":"Vmo19ThCZ0vi"}},{"cell_type":"code","source":["import numpy\n","from onnx import numpy_helper, TensorProto\n","from onnx.helper import (\n","    make_model, make_node, make_graph,\n","    make_tensor_value_info)\n","from onnx.checker import check_model\n","\n","def shape2tuple(shape):\n","    return tuple(getattr(d, 'dim_value', 0) for d in shape.dim)\n","\n","X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n","A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n","B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n","Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n","\n","node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n","node2 = make_node('Add', ['XA', 'B'], ['Y'])\n","\n","graph = make_graph([node1, node2], 'lr', [X, A, B], [Y])\n","onnx_model = make_model(graph)\n","check_model(onnx_model)\n","\n","# The serialization\n","with open(\"linear_regression.onnx\", \"wb\") as f:\n","    f.write(onnx_model.SerializeToString())"],"metadata":{"id":"seVKsta8g3dq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initializers\n","value = numpy.array([0.5, -0.6], dtype=numpy.float32)\n","A = numpy_helper.from_array(value, name='A')\n","\n","value = numpy.array([0.4], dtype=numpy.float32)\n","C = numpy_helper.from_array(value, name='C')\n","\n","# the part which does not change\n","X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n","Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n","node1 = make_node('MatMul', ['X', 'A'], ['AX'])\n","node2 = make_node('Add', ['AX', 'C'], ['Y'])\n","graph = make_graph([node1, node2], 'lr', [X], [Y], [A, C])\n","onnx_model = make_model(graph)\n","check_model(onnx_model)\n","\n","print('** initializer **')\n","for init in onnx_model.graph.initializer:\n","    print(init)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARvdt67qZ4DT","executionInfo":{"status":"ok","timestamp":1718314348973,"user_tz":-420,"elapsed":369,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"1208cf33-9520-4ad4-c894-138e07cbf4e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["** initializer **\n","dims: 2\n","data_type: 1\n","name: \"A\"\n","raw_data: \"\\000\\000\\000?\\232\\231\\031\\277\"\n","\n","dims: 1\n","data_type: 1\n","name: \"C\"\n","raw_data: \"\\315\\314\\314>\"\n","\n"]}]},{"cell_type":"markdown","source":["$$\n","Y = XA + B \\\\\n","y = \\text{Add}(\\text{MatMul}(X, \\text{Transpose}(A)) + B)\n","$$"],"metadata":{"id":"h1GcnYWlaTJa"}},{"cell_type":"code","source":["# unchanged\n","X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n","A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n","B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n","Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n","\n","# added\n","node_transpose = make_node('Transpose', ['A'], ['tA'], perm=[1, 0])\n","\n","# unchanged except A is replaced by tA\n","node1 = make_node('MatMul', ['X', 'tA'], ['XA'])\n","node2 = make_node('Add', ['XA', 'B'], ['Y'])\n","\n","# node_transpose is added to the list\n","graph = make_graph([node_transpose, node1, node2],\n","                   'lr', [X, A, B], [Y])\n","onnx_model = make_model(graph)\n","check_model(onnx_model)\n","\n","# the work is done, let's display it...\n","print(onnx_model)"],"metadata":{"id":"xXLYLZzRaRai","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import onnx\n","import pprint\n","pprint.pprint([k for k in dir(onnx.helper)\n","               if k.startswith('make')])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Sj_UWOUaj27","executionInfo":{"status":"ok","timestamp":1718314144529,"user_tz":-420,"elapsed":382,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"87a0a2e0-ff0e-4451-d3ee-a42639836759"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['make_attribute',\n"," 'make_attribute_ref',\n"," 'make_empty_tensor_value_info',\n"," 'make_function',\n"," 'make_graph',\n"," 'make_map',\n"," 'make_map_type_proto',\n"," 'make_model',\n"," 'make_model_gen_version',\n"," 'make_node',\n"," 'make_operatorsetid',\n"," 'make_opsetid',\n"," 'make_optional',\n"," 'make_optional_type_proto',\n"," 'make_sequence',\n"," 'make_sequence_type_proto',\n"," 'make_sparse_tensor',\n"," 'make_sparse_tensor_type_proto',\n"," 'make_sparse_tensor_value_info',\n"," 'make_tensor',\n"," 'make_tensor_sequence_value_info',\n"," 'make_tensor_type_proto',\n"," 'make_tensor_value_info',\n"," 'make_training_info',\n"," 'make_value_info']\n"]}]},{"cell_type":"markdown","source":["### Metadata, opset"],"metadata":{"id":"cI7Xlyp5aoAq"}},{"cell_type":"code","source":["from onnx import load, helper\n","\n","with open(\"linear_regression.onnx\", \"rb\") as f:\n","    onnx_model = load(f)\n","\n","for field in ['doc_string', 'domain', 'functions',\n","              'ir_version', 'metadata_props', 'model_version',\n","              'opset_import', 'producer_name', 'producer_version',\n","              'training_info']:\n","    print(field, getattr(onnx_model, field))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eba1U7DBank6","executionInfo":{"status":"ok","timestamp":1718314397397,"user_tz":-420,"elapsed":379,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"cd0ca05c-4a97-431f-955e-8adea83a2594"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["doc_string \n","domain \n","functions []\n","ir_version 10\n","metadata_props []\n","model_version 0\n","opset_import [version: 21\n","]\n","producer_name \n","producer_version \n","training_info []\n"]}]},{"cell_type":"code","source":["with open(\"linear_regression.onnx\", \"rb\") as f:\n","    onnx_model = load(f)\n","\n","print(\"ir_version:\", onnx_model.ir_version)\n","for opset in onnx_model.opset_import:\n","    print(\"opset domain=%r version=%r\" % (opset.domain, opset.version))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTXHRlh9a0uK","executionInfo":{"status":"ok","timestamp":1718314374871,"user_tz":-420,"elapsed":449,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"02ea51fa-edb2-47fb-c0bf-80ebcbff371e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ir_version: 10\n","opset domain='' version=21\n"]}]},{"cell_type":"code","source":["with open(\"linear_regression.onnx\", \"rb\") as f:\n","    onnx_model = load(f)\n","\n","del onnx_model.opset_import[:]\n","opset = onnx_model.opset_import.add() # comment this to see what happens\n","opset.domain = 'This is domain'\n","opset.version = 14\n","\n","for opset in onnx_model.opset_import:\n","    print(\"opset domain=%r version=%r\" % (opset.domain, opset.version))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EBUb7kflbHfq","executionInfo":{"status":"ok","timestamp":1718314380152,"user_tz":-420,"elapsed":3,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"5dc90c8c-b25f-4dfa-e6d9-7e96954136b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["opset domain='This is domain' version=14\n"]}]},{"cell_type":"code","source":["with open(\"linear_regression.onnx\", \"rb\") as f:\n","    onnx_model = load(f)\n","\n","onnx_model.model_version = 15\n","onnx_model.producer_name = \"something\"\n","onnx_model.producer_version = \"some other thing\"\n","onnx_model.doc_string = \"documentation about this model\"\n","prop = onnx_model.metadata_props\n","\n","data = dict(key1=\"value1\", key2=\"value2\")\n","helper.set_model_props(onnx_model, data)\n","\n","print(onnx_model)"],"metadata":{"id":"-offGDhNbWWy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Subgraph: test and loops"],"metadata":{"id":"eFsNZRZXbrgK"}},{"cell_type":"code","source":["!pip install onnxruntime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SOVDNuEDhWS6","executionInfo":{"status":"ok","timestamp":1718314443071,"user_tz":-420,"elapsed":10752,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"4bb1c0f0-7168-430f-abc1-fc2882ed2097"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnxruntime\n","  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.18.0\n"]}]},{"cell_type":"code","source":["import numpy\n","import onnx\n","from onnx.helper import (\n","    make_node, make_graph, make_model, make_tensor_value_info)\n","from onnx.numpy_helper import from_array\n","from onnx.checker import check_model\n","from onnxruntime import InferenceSession\n","\n","# initializers\n","value = numpy.array([0], dtype=numpy.float32)\n","zero = from_array(value, name='zero')\n","\n","# Same as before, X is the input, Y is the output.\n","X = make_tensor_value_info('X', onnx.TensorProto.FLOAT, [None, None])\n","Y = make_tensor_value_info('Y', onnx.TensorProto.FLOAT, [None])\n","\n","# The node building the condition. The first one\n","# sum over all axes.\n","rsum = make_node('ReduceSum', ['X'], ['rsum'])\n","# The second compares the result to 0.\n","cond = make_node('Greater', ['rsum', 'zero'], ['cond'])\n","\n","# Builds the graph is the condition is True.\n","# Input for then\n","then_out = make_tensor_value_info(\n","    'then_out', onnx.TensorProto.FLOAT, None)\n","# The constant to return.\n","then_cst = from_array(numpy.array([1]).astype(numpy.float32))\n","\n","# The only node.\n","then_const_node = make_node(\n","    'Constant', inputs=[],\n","    outputs=['then_out'],\n","    value=then_cst, name='cst1')\n","\n","# And the graph wrapping these elements.\n","then_body = make_graph(\n","    [then_const_node], 'then_body', [], [then_out])\n","\n","# Same process for the else branch.\n","else_out = make_tensor_value_info(\n","    'else_out', onnx.TensorProto.FLOAT, [5])\n","else_cst = from_array(numpy.array([-1]).astype(numpy.float32))\n","\n","else_const_node = make_node(\n","    'Constant', inputs=[],\n","    outputs=['else_out'],\n","    value=else_cst, name='cst2')\n","\n","else_body = make_graph(\n","    [else_const_node], 'else_body',\n","    [], [else_out])\n","\n","# Finally the node If taking both graphs as attributes.\n","if_node = onnx.helper.make_node(\n","    'If', ['cond'], ['Y'],\n","    then_branch=then_body,\n","    else_branch=else_body)\n","\n","# The final graph.\n","graph = make_graph([rsum, cond, if_node], 'if', [X], [Y], [zero])\n","onnx_model = make_model(graph)\n","check_model(onnx_model)\n","\n","# Let's freeze the opset.\n","del onnx_model.opset_import[:]\n","opset = onnx_model.opset_import.add()\n","opset.domain = ''\n","opset.version = 15\n","onnx_model.ir_version = 8\n","\n","# Save.\n","with open(\"onnx_if_sign.onnx\", \"wb\") as f:\n","    f.write(onnx_model.SerializeToString())\n","\n","# Let's see the output.\n","sess = InferenceSession(onnx_model.SerializeToString(),\n","                        providers=[\"CPUExecutionProvider\"])\n","\n","x = numpy.ones((3, 2), dtype=numpy.float32)\n","res = sess.run(None, {'X': x})\n","\n","# It works.\n","print(\"result\", res)\n","print()\n","\n","# Some display.\n","print(onnx_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MVrLAxM7brO6","executionInfo":{"status":"ok","timestamp":1718314469518,"user_tz":-420,"elapsed":409,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"235c82ad-a356-40a5-b1ee-bb2f9a61b49e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["result [array([1.], dtype=float32)]\n","\n","ir_version: 8\n","graph {\n","  node {\n","    input: \"X\"\n","    output: \"rsum\"\n","    op_type: \"ReduceSum\"\n","  }\n","  node {\n","    input: \"rsum\"\n","    input: \"zero\"\n","    output: \"cond\"\n","    op_type: \"Greater\"\n","  }\n","  node {\n","    input: \"cond\"\n","    output: \"Y\"\n","    op_type: \"If\"\n","    attribute {\n","      name: \"else_branch\"\n","      g {\n","        node {\n","          output: \"else_out\"\n","          name: \"cst2\"\n","          op_type: \"Constant\"\n","          attribute {\n","            name: \"value\"\n","            t {\n","              dims: 1\n","              data_type: 1\n","              raw_data: \"\\000\\000\\200\\277\"\n","            }\n","            type: TENSOR\n","          }\n","        }\n","        name: \"else_body\"\n","        output {\n","          name: \"else_out\"\n","          type {\n","            tensor_type {\n","              elem_type: 1\n","              shape {\n","                dim {\n","                  dim_value: 5\n","                }\n","              }\n","            }\n","          }\n","        }\n","      }\n","      type: GRAPH\n","    }\n","    attribute {\n","      name: \"then_branch\"\n","      g {\n","        node {\n","          output: \"then_out\"\n","          name: \"cst1\"\n","          op_type: \"Constant\"\n","          attribute {\n","            name: \"value\"\n","            t {\n","              dims: 1\n","              data_type: 1\n","              raw_data: \"\\000\\000\\200?\"\n","            }\n","            type: TENSOR\n","          }\n","        }\n","        name: \"then_body\"\n","        output {\n","          name: \"then_out\"\n","          type {\n","            tensor_type {\n","              elem_type: 1\n","            }\n","          }\n","        }\n","      }\n","      type: GRAPH\n","    }\n","  }\n","  name: \"if\"\n","  initializer {\n","    dims: 1\n","    data_type: 1\n","    name: \"zero\"\n","    raw_data: \"\\000\\000\\000\\000\"\n","  }\n","  input {\n","    name: \"X\"\n","    type {\n","      tensor_type {\n","        elem_type: 1\n","        shape {\n","          dim {\n","          }\n","          dim {\n","          }\n","        }\n","      }\n","    }\n","  }\n","  output {\n","    name: \"Y\"\n","    type {\n","      tensor_type {\n","        elem_type: 1\n","        shape {\n","          dim {\n","          }\n","        }\n","      }\n","    }\n","  }\n","}\n","opset_import {\n","  domain: \"\"\n","  version: 15\n","}\n","\n"]}]},{"cell_type":"markdown","source":["### Function"],"metadata":{"id":"0k9diOeMdmwJ"}},{"cell_type":"code","source":["import numpy\n","from onnx import numpy_helper, TensorProto\n","from onnx.helper import (\n","    make_model, make_node, set_model_props, make_tensor,\n","    make_graph, make_tensor_value_info, make_opsetid,\n","    make_function)\n","from onnx.checker import check_model\n","\n","new_domain = 'custom'\n","opset_imports = [make_opsetid(\"\", 14), make_opsetid(new_domain, 1)]\n","\n","# Let's define a function for a linear regression\n","\n","node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n","node2 = make_node('Add', ['XA', 'B'], ['Y'])\n","\n","linear_regression = make_function(\n","    new_domain,            # domain name\n","    'LinearRegression',     # function name\n","    ['X', 'A', 'B'],        # input names\n","    ['Y'],                  # output names\n","    [node1, node2],         # nodes\n","    opset_imports,          # opsets\n","    [])                     # attribute names\n","\n","# Let's use it in a graph.\n","\n","X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n","A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n","B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n","Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n","\n","graph = make_graph(\n","    [make_node('LinearRegression', ['X', 'A', 'B'], ['Y1'], domain=new_domain),\n","     make_node('Abs', ['Y1'], ['Y'])],\n","    'example',\n","    [X, A, B], [Y])\n","\n","onnx_model = make_model(\n","    graph, opset_imports=opset_imports,\n","    functions=[linear_regression])  # functions to add)\n","check_model(onnx_model)\n","\n","# the work is done, let's display it...\n","print(onnx_model)"],"metadata":{"id":"ywMRs02gdmZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Function with attributes"],"metadata":{"id":"uhkGM1zzd9ty"}},{"cell_type":"code","source":["import numpy\n","from onnx import numpy_helper, TensorProto, AttributeProto\n","from onnx.helper import (\n","    make_model, make_node, set_model_props, make_tensor,\n","    make_graph, make_tensor_value_info, make_opsetid,\n","    make_function)\n","from onnx.checker import check_model\n","\n","new_domain = 'custom'\n","opset_imports = [make_opsetid(\"\", 14), make_opsetid(new_domain, 1)]\n","\n","# Let's define a function for a linear regression\n","# The first step consists in creating a constant\n","# equal to the input parameter of the function.\n","cst = make_node('Constant',  [], ['B'])\n","\n","att = AttributeProto()\n","att.name = \"value\"\n","\n","# This line indicates the value comes from the argument\n","# named 'bias' the function is given.\n","att.ref_attr_name = \"bias\"\n","att.type = AttributeProto.TENSOR\n","cst.attribute.append(att)\n","\n","node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n","node2 = make_node('Add', ['XA', 'B'], ['Y'])\n","\n","linear_regression = make_function(\n","    new_domain,            # domain name\n","    'LinearRegression',     # function name\n","    ['X', 'A'],             # input names\n","    ['Y'],                  # output names\n","    [cst, node1, node2],    # nodes\n","    opset_imports,          # opsets\n","    [\"bias\"])               # attribute names\n","\n","# Let's use it in a graph.\n","\n","X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n","A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n","B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n","Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n","\n","graph = make_graph(\n","    [make_node('LinearRegression', ['X', 'A'], ['Y1'], domain=new_domain,\n","               # bias is now an argument of the function and is defined as a tensor\n","               bias=make_tensor('former_B', TensorProto.FLOAT, [1], [0.67])),\n","     make_node('Abs', ['Y1'], ['Y'])],\n","    'example',\n","    [X, A], [Y])\n","\n","onnx_model = make_model(\n","    graph, opset_imports=opset_imports,\n","    functions=[linear_regression])  # functions to add)\n","check_model(onnx_model)\n","\n","# the work is done, let's display it...\n","print(onnx_model)"],"metadata":{"id":"GfMJpVf8eALq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation and Runtime"],"metadata":{"id":"rv-rYytdeWmi"}},{"cell_type":"code","source":["import numpy\n","from onnx import numpy_helper, TensorProto\n","from onnx.helper import (\n","    make_model, make_node, set_model_props, make_tensor,\n","    make_graph, make_tensor_value_info)\n","from onnx.checker import check_model\n","from onnx.reference import ReferenceEvaluator\n","\n","X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n","A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n","B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n","Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n","node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n","node2 = make_node('Add', ['XA', 'B'], ['Y'])\n","graph = make_graph([node1, node2], 'lr', [X, A, B], [Y])\n","onnx_model = make_model(graph)\n","check_model(onnx_model)\n","\n","sess = ReferenceEvaluator(onnx_model)\n","\n","x = numpy.random.randn(4, 2).astype(numpy.float32)\n","a = numpy.random.randn(2, 1).astype(numpy.float32)\n","b = numpy.random.randn(1, 1).astype(numpy.float32)\n","feeds = {'X': x, 'A': a, 'B': b}\n","\n","print(sess.run(None, feeds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_U1iDA4eaPa","executionInfo":{"status":"ok","timestamp":1718314495036,"user_tz":-420,"elapsed":410,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"eb2ee665-6cdd-4160-9577-cabd9f232013"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[array([[-1.6823176 ],\n","       [-2.07294   ],\n","       [-0.3106615 ],\n","       [-0.17735608]], dtype=float32)]\n"]}]},{"cell_type":"code","source":["import numpy\n","from onnx import numpy_helper, TensorProto\n","from onnx.helper import (\n","    make_model, make_node, set_model_props, make_tensor,\n","    make_graph, make_tensor_value_info)\n","from onnx.checker import check_model\n","from onnx.reference import ReferenceEvaluator\n","\n","X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n","A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n","B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n","Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n","node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n","node2 = make_node('Add', ['XA', 'B'], ['Y'])\n","graph = make_graph([node1, node2], 'lr', [X, A, B], [Y])\n","onnx_model = make_model(graph)\n","check_model(onnx_model)\n","\n","for verbose in [1, 2, 3, 4]:\n","    print()\n","    print(f\"------ verbose={verbose}\")\n","    print()\n","    sess = ReferenceEvaluator(onnx_model, verbose=verbose)\n","\n","    x = numpy.random.randn(4, 2).astype(numpy.float32)\n","    a = numpy.random.randn(2, 1).astype(numpy.float32)\n","    b = numpy.random.randn(1, 1).astype(numpy.float32)\n","    feeds = {'X': x, 'A': a, 'B': b}\n","\n","    print(sess.run(None, feeds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGmtG9wuek5S","executionInfo":{"status":"ok","timestamp":1718045269319,"user_tz":-420,"elapsed":429,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"4daf0ef3-f54e-4a07-c780-b6f727877579"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","------ verbose=1\n","\n","[array([[ 1.6946731 ],\n","       [-0.08133245],\n","       [ 3.2676148 ],\n","       [ 1.3359237 ]], dtype=float32)]\n","\n","------ verbose=2\n","\n","MatMul(X, A) -> XA\n","Add(XA, B) -> Y\n","[array([[-0.25301543],\n","       [-0.03467366],\n","       [-0.35421404],\n","       [-0.18787482]], dtype=float32)]\n","\n","------ verbose=3\n","\n"," +I X: float32:(4, 2) in [-2.7870829105377197, 1.5124919414520264]\n"," +I A: float32:(2, 1) in [0.2143881767988205, 1.8981961011886597]\n"," +I B: float32:(1, 1) in [1.3698805570602417, 1.3698805570602417]\n","MatMul(X, A) -> XA\n"," + XA: float32:(4, 1) in [-1.3159455060958862, 2.936558485031128]\n","Add(XA, B) -> Y\n"," + Y: float32:(4, 1) in [0.05393505096435547, 4.30643892288208]\n","[array([[1.4402075 ],\n","       [4.306439  ],\n","       [3.020172  ],\n","       [0.05393505]], dtype=float32)]\n","\n","------ verbose=4\n","\n"," +I X: float32:(4, 2):0.9752460718154907,-1.4864572286605835,0.03714435547590256,0.6927376985549927,-0.7181068062782288...\n"," +I A: float32:(2, 1):[0.703309178352356, 0.443999320268631]\n"," +I B: float32:(1, 1):[0.4355386793613434]\n","MatMul(X, A) -> XA\n"," + XA: float32:(4, 1):[0.025913476943969727, 0.3336990475654602, -0.5876479148864746, -0.39018741250038147]\n","Add(XA, B) -> Y\n"," + Y: float32:(4, 1):[0.4614521563053131, 0.769237756729126, -0.15210923552513123, 0.045351266860961914]\n","[array([[ 0.46145216],\n","       [ 0.76923776],\n","       [-0.15210924],\n","       [ 0.04535127]], dtype=float32)]\n"]}]},{"cell_type":"markdown","source":["## ONNX Runtime\n","\n","**Khái niệm:** ONNX Runtime is a performance-focused engine for ONNX models, which inferences efficiently\n","across multiple platforms and hardware (Windows, Linux, and Mac and on\n","both CPUs and GPUs)\n","\n","Ta không cần viết code với \"ngôn ngữ\" onnx từ đầu một cách thủ công lại như trên vì ta có thể sử dụng `ONNX Runtime` để chuyển đổi kiến trúc mô hình từ các framework `TensorFlow/Pytorch`. Hiện nay `ONNX Runtime` đã có thể tích hợp sẵn trong các thư viện ML/DL, ví dụ `keras2onnx`, `tf2onnx`, `torch.onnx`. Và việc inference đã được hỗ trợ bởi `onnxruntime.InferenceSession`."],"metadata":{"id":"XR3iVeRngLYi"}},{"cell_type":"markdown","source":["### Sử dụng torch.onnx.export\n","Ta có hai bước cần thực hiện khi chuyển đổi sang onnx là 1.conversion và 2. inference. Ví dụ mẫu với mô hình DenseNet pretrained trên bộ dữ liệu ImageNet1K với thư viện torchvision."],"metadata":{"id":"b6efKh6wiUZ2"}},{"cell_type":"markdown","source":["#### Bước 1: Convert model"],"metadata":{"id":"OLGpes5Klfcx"}},{"cell_type":"code","source":["model = torchvision.models.densenet161(weights=\"DenseNet161_Weights.IMAGENET1K_V1\", progress=True)"],"metadata":{"id":"zuNGvWt_igTR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718314981199,"user_tz":-420,"elapsed":4424,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"ba51b89e-b269-49b9-afb8-7698b6e1a54f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n","100%|██████████| 110M/110M [00:02<00:00, 40.0MB/s]\n"]}]},{"cell_type":"markdown","source":["**Constant folding** is an optimization technique used in compiler theory, including the context of neural network models. Here’s what it does:\n","\n","- Identify Constants: The process scans the computational graph to identify operations that involve only constants.\n","- Compute at Export Time: Instead of keeping these constant computations as part of the graph, the values are precomputed during the export process.\n","- Simplify the Graph: The precomputed values replace the original operations in the graph, simplifying it and potentially reducing the computational load during runtime.\n","\n","**Benefits**:\n","- Reduced Computation: By precomputing constant values, the model requires fewer computations during inference, which can lead to faster execution.\n","- Smaller Model Size: Simplifying the graph by removing unnecessary operations can reduce the overall size of the model.\n","- Optimization: It helps in optimizing the model for better performance on various hardware by eliminating redundant calculations."],"metadata":{"id":"Got77bGNkkgy"}},{"cell_type":"code","source":["input = torch.randn(12, 3, 224, 224, requires_grad=True)\n","\n","torch.onnx.export(\n","    model, input, \"densenet161.onnx\", export_params=False, opset_version=10,\n","    input_names=[\"input\"], output_names=[\"output\"], do_constant_folding=True,\n","    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",")"],"metadata":{"id":"QkYs2xNKjeBq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Bước 2: Inference với converted model"],"metadata":{"id":"DvWN1xhBldby"}},{"cell_type":"code","source":["!nvcc --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VBxrDyURnoeC","executionInfo":{"status":"ok","timestamp":1718316082272,"user_tz":-420,"elapsed":5,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"daff242d-f8bb-41d4-8f27-93baf108dcef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n"]}]},{"cell_type":"markdown","source":["Kiểm tra cuda version là 12 nên dùng ONNX Runtime Azure"],"metadata":{"id":"_Kxgjhvsnt4L"}},{"cell_type":"code","source":["!pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/"],"metadata":{"id":"CC4gqG1om7Us"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import onnxruntime as ort\n","\n","# Preprocessing\n","def preprocess(path):\n","  augment = transforms.Compose([\n","      transforms.Resize(256),\n","      transforms.CenterCrop(224),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","  ])\n","\n","  image = Image.open(path)\n","  image = augment(image)\n","  return image.unsqueeze(0).numpy()\n","\n","inputs = preprocess(\"/content/junco.png\")\n","\n","# Prepare providers\n","providers = [\n","    'TensorrtExecutionProvider',\n","    'CUDAExecutionProvider',\n","    'CPUExecutionProvider'\n","]\n","\n","ort_session = ort.InferenceSession(\"/content/densenet161.onnx\", providers=providers)\n","inp = {ort_session.get_inputs()[0].name: inputs}\n","out = ort_session.run(None, inp)\n","\n","# Postprocessing\n","def postprocess(out):\n","  idx = np.argmax(out[0])\n","  return idx\n","\n","print(postprocess(out))"],"metadata":{"id":"a-L5eVlbloNh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["List được sắp xếp theo thứ tự ưu tiên, ngoài ra ta có thể specify thêm thông tin cho từng providers, đọc thêm tại [documentation](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html). Lưu ý để thực hiện inference với TensorRT, ta cần install đúng version của `cuda, cudnn, tensorrt`, tham khảo documentation này để tìm hiểu về Docker để cài TensorRT thành công. Dưới đây là một ví dụ về provider config cho TensorRT backend.\n","\n","```python\n","providers = [\n","    ('TensorrtExecutionProvider', {\n","        'device_id': 0,                           # Select GPU to execute\n","        'trt_max_workspace_size': 2147483648,     # Set GPU memory usage limit\n","        'trt_fp16_enable': True,                  # Enable FP16 precision for faster inference  \n","        'trt_engine_cache_enable': True,\n","        'trt_engine_cache_path': 'Engine/onnx_models',\n","    }),\n","]\n","```"],"metadata":{"id":"Jq6l5hfVx10y"}},{"cell_type":"markdown","source":["### Cách simpify mô hình onnx với onnxsim\n","\n","Cơ chế khởi tạo sơ đồ phép tính (graph) của ONNX đôi khi có cấu trúc quá phức tạp và có những phần không cần thiết, ta có thể đơn giản hóa để làm gọn graph và làm mô hình nhẹ bằng thư viện [onnxsim](https://github.com/daquexian/onnx-simplifier).\n","- Cài thư viện"],"metadata":{"id":"OYLSQFXlqXwp"}},{"cell_type":"code","source":["!pip install onnxsim"],"metadata":{"id":"aYDkusGwqZV5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Thực hiện trên terminal"],"metadata":{"id":"WEt0o7ito54G"}},{"cell_type":"code","source":["!onnxsim /content/densenet161.onnx /content/simplified_densenet161.onnx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XhubD5VRo8Cc","executionInfo":{"status":"ok","timestamp":1718316446103,"user_tz":-420,"elapsed":4479,"user":{"displayName":"Kyle Paul","userId":"03868291413443864817"}},"outputId":"d5e50180-8445-415a-8464-5e7b2a263744"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Simplifying\u001b[33m...\u001b[0m\n","Finish! Here is the difference:\n","┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n","│ AveragePool        │ 3              │ 3                │\n","│ BatchNormalization │ 82             │ 82               │\n","│ Concat             │ 82             │ \u001b[1;32m78              \u001b[0m │\n","│ Constant           │ 569            │ 569              │\n","│ Conv               │ 160            │ 160              │\n","│ Flatten            │ 1              │ 1                │\n","│ Gemm               │ 1              │ 1                │\n","│ GlobalAveragePool  │ 1              │ 1                │\n","│ MaxPool            │ 1              │ 1                │\n","│ Relu               │ 161            │ 161              │\n","│ Model Size         │ 110.3MiB       │ 110.3MiB         │\n","└────────────────────┴────────────────┴──────────────────┘\n"]}]},{"cell_type":"code","source":["import onnx\n","from onnxsim import simplify\n","\n","model = onnx.load(\"/content/densenet161.onnx\")\n","model_simp, check = simplify(model)\n","\n","assert check, \"Simplified ONNX model could not be validated\"\n","onnx.save(model_simp, \"/content/densenet161_sim.onnx\")"],"metadata":{"id":"cEYvK5l3p_vB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ONNX with OpenVINO"],"metadata":{"id":"LGAep9BjpkCV"}},{"cell_type":"code","source":["!pip install openvino --no-deps"],"metadata":{"id":"lSq_vM6vpqw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import openvino as ov\n","\n","input = torch.tensor(12, 3, 224, 224)\n","\n","core = ov.Core()\n","compiled_model = core.compile_model(\"model.onnx\", \"CPU\")\n","infer_request = compiled_model.create_infer_request()"],"metadata":{"id":"_SRp0w0Epmxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Warmup step\n","input_tensor = ov.Tensor(array=inputs, shared_memory=True)\n","infer_request.set_input_tensor(input_tensor)"],"metadata":{"id":"fvrv45Jh5AEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%timeit\n","output_tensor = infer_request.infer()"],"metadata":{"id":"fcQZpHPn5A_c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Precision and Quantization\n","Có hai cách để nén một mô hình nặng trở nên nhẹ hơn: low-rank precision và quantization.\n"],"metadata":{"id":"-efzMaf508AT"}},{"cell_type":"markdown","source":["### Precision:\n","- **Half-precision**: chuyển bộ trọng số $W$ từ kiểu dữ liệu `float32` (FP32) sang kiểu dữ liệu `float16` (FP16). Điều này giúp tối ưu một nửa bộ nhớ (reduced Memory Usage) và tăng hiệu năng (increased throughput) của hardware thích hợp với tính toán FP16, nhưng sẽ mất precision nên cần benchmark lại để đánh giá.\n","- **Mixed precision**: kết hợp một cách cân bằng giữa hai loại FP16 và FP32 để vừa tối ưu bộ nhớ nhưng vẫn tối thiểu sai số nhất có thể\n","    Sử dụng mixed precision với `neural-compressor.mix_precision` (intel)\n","\n","\n","Tham khảo bảng hỗ trợ của neural-compressor:\n","<table class=\"center\">\n","<thead>\n","    <tr>\n","        <th>Framework</th>\n","        <th>Backend</th>\n","        <th>Backend Library</th>\n","        <th>Backend Value</th>\n","        <th>Support Device(cpu as default)</th>\n","        <th>Support BF16</th>\n","        <th>Support FP16</th>\n","    </tr>\n","</thead>\n","<tbody>\n","    <tr>\n","        <td rowspan=\"2\" align=\"left\">PyTorch</td>\n","        <td align=\"left\">FX</td>\n","        <td align=\"left\">FBGEMM</td>\n","        <td align=\"left\">\"default\"</td>\n","        <td align=\"left\">cpu</td>\n","        <td align=\"left\">&#10004;</td>\n","        <td align=\"left\">&#10006;</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">IPEX</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"ipex\"</td>\n","        <td align=\"left\">cpu</td>\n","        <td align=\"left\">&#10004;</td>\n","        <td align=\"left\">&#10006;</td>\n","    </tr>\n","    <tr>\n","        <td rowspan=\"4\" align=\"left\">ONNX Runtime</td>\n","        <td align=\"left\">CPUExecutionProvider</td>\n","        <td align=\"left\">MLAS</td>\n","        <td align=\"left\">\"default\"</td>\n","        <td align=\"left\">cpu</td>\n","        <td align=\"left\">&#10006;</td>\n","        <td align=\"left\">&#10006;</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">TensorrtExecutionProvider</td>\n","        <td align=\"left\">TensorRT</td>\n","        <td align=\"left\">\"onnxrt_trt_ep\"</td>\n","        <td align=\"left\">gpu</td>\n","        <td align=\"left\">&#10006;</td>\n","        <td align=\"left\">&#10006;</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">CUDAExecutionProvider</td>\n","        <td align=\"left\">CUDA</td>\n","        <td align=\"left\">\"onnxrt_cuda_ep\"</td>\n","        <td align=\"left\">gpu</td>\n","        <td align=\"left\">&#10004;</td>\n","        <td align=\"left\">&#10004;</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">DnnlExecutionProvider</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"onnxrt_dnnl_ep\"</td>\n","        <td align=\"left\">cpu</td>\n","        <td align=\"left\">&#10004;</td>\n","        <td align=\"left\">&#10006;</td>\n","    </tr>\n","    <tr>\n","        <td rowspan=\"2\" align=\"left\">Tensorflow</td>\n","        <td align=\"left\">Tensorflow</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"default\"</td>\n","        <td align=\"left\">cpu</td>\n","        <td align=\"left\">&#10004;</td>\n","        <td align=\"left\">&#10006;</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">ITEX</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"itex\"</td>\n","        <td align=\"left\">cpu | gpu</td>\n","        <td align=\"left\">&#10004;</td>\n","        <td align=\"left\">&#10006;</td>\n","    </tr>  \n","    <tr>\n","        <td align=\"left\">MXNet</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"default\"</td>\n","        <td align=\"left\">cpu</td>\n","        <td align=\"left\">&#10004;</td>\n","        <td align=\"left\">&#10006;</td>\n","    </tr>\n","</tbody>\n","</table>"],"metadata":{"id":"CVwse16z1LLV"}},{"cell_type":"code","source":["!pip install neural-compressor"],"metadata":{"id":"YT5HcReW1OUM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from neural_compressor import mix_precision\n","from neural_compressor.config import MixedPrecisionConfig"],"metadata":{"id":"hz65BNbQ1RyO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compress model trực tiếp từ model pytorch (chỉ có CPU)"],"metadata":{"id":"41A3S8eE1ePj"}},{"cell_type":"code","source":["conf = MixedPrecisionConfig(\n","    backend=\"ipex\",\n","    device=\"cpu\",\n","    precisions=\"bf16\",\n",")\n","torch_model = torch.load(\"/content/swin-b.pt\")\n","converted_model = mix_precision.fit(torch_model, conf=conf)"],"metadata":{"id":"kCS3j_6j1b5k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compress model.onnx với GPU"],"metadata":{"id":"IaUt7Mgy1fzN"}},{"cell_type":"code","source":["conf = MixedPrecisionConfig(\n","    backend=\"onnxrt_cuda_ep\",\n","    device=\"gpu\",\n","    precisions=\"fp16\",\n",") # chỉ sử dụng được với GPU cho model ỏ onnx format.\n","\n","onnx_model = onnx.load(\"/content/densenet161_sim.onnx\")\n","converted_model = mix_precision.fit(onnx_model, conf=conf)\n","converted_model.save(\"mixed_precision_densenet161_sim.onnx\")"],"metadata":{"id":"zSVmGAxA1yGz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compress model.onnx với CPU"],"metadata":{"id":"fuUUPAPQ10cU"}},{"cell_type":"code","source":["conf = MixedPrecisionConfig(\n","    backend=\"onnxrt_dnnl_ep\",\n","    device=\"cpu\",\n","    precisions=\"bf16\",\n",")\n","\n","onnx_model = onnx.load(\"/content/densenet161_sim.onnx\")\n","converted_model = mix_precision.fit(onnx_model, conf=conf)\n","converted_model.save(\"mixed_precision_densenet_cpu.onnx\")"],"metadata":{"id":"nDGgnQ6O1zZ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Quantization\n","Quantization là một kỹ thuật tối ưu nâng cao để tăng tốc độ inference/training. Nó giúp giảm số bit lưu trữ bằng cách chuyển số thực sang dạng số nguyên `int8`, `int4` nhưng không làm mất đi accuracy. Có thể phân loại quantization bằng hai cách.\n","- Chia theo đặc tính: Affine Quantization (asymmetric) và Scale Quantization (symmetric).\n","- Chia theo cách sử dụng: Post-Training Dynamic Quantization, Post-Training Static Quantization, Quantization-Aware Training."],"metadata":{"id":"snAQ3Qcd3b5W"}},{"cell_type":"markdown","source":["#### Sử dụng\n","Sử dụng trực tiếp `torch.quantization`: \\\n","**Post-Training Quantization**"],"metadata":{"id":"6w0tOQpS3kPC"}},{"cell_type":"code","source":["import torch\n","\n","model = torch.load('/content/swin-b.pt')\n","model.eval()\n","\n","quantized_model = torch.quantization.quantize_dynamic(\n","    model, {torch.nn.Linear}, dtype=torch.qint8\n",")\n","\n","torch.save(quantized_model, 'quantized-swin-b.pt')"],"metadata":{"id":"CuZ7esSk3qM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Quantization-Aware Training**\n","\n","```python\n","# Enable quantization-aware training\n","model.qconfig = torch.quantization.default_qconfig\n","\n","# Convert the model to quantized version\n","quantized_model = torch.quantization.convert(model, inplace=False)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(quantized_model.parameters(), lr=0.01, momentum=0.9)\n","\n","# Train the model\n","for epoch in range(5):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # Get the inputs and labels\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = quantized_model(inputs)\n","\n","        # Compute the loss\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Optimize\n","        optimizer.step()\n","\n","# Evaluate the model\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = quantized_model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n","\n","# Save the model\n","torch.save(quantized_model, 'mnist_model_quantized.pt')\n","```"],"metadata":{"id":"e7cMz9KW32M8"}},{"cell_type":"markdown","source":["Sử dụng `neural-compressor.quantization:`"],"metadata":{"id":"1o56ZBaQ4AyT"}},{"cell_type":"code","source":["from neural_compressor.config import PostTrainingQuantConfig\n","from neural_compressor import quantization"],"metadata":{"id":"2ufZhM7I4GYc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Post-Training Quantization (w/o Accuracy Aware Tuning)\n","```python\n","val_dataset = ...\n","val_loader = ...\n","model = ...\n","\n","conf = (PostTrainingQuantConfig())\n","quantized_model = quantization.fit(\n","    model=model,\n","    conf=conf,\n","    calib_dataloader=val_dataloader,\n",")\n","```\n","Post-Training Quantization (with Accuracy Aware Tuning)\n","```python\n","def validate(val_loader, model, criterion, args):\n","    ...\n","    return top1.avg\n","\n","quantized_model = quantization.fit(\n","    model=model,\n","    conf=conf,\n","    calib_dataloader=val_dataloader,\n","    eval_func=validate,\n",")\n","```\n","\n","Quantization-Aware Training\n","```python\n","from neural_compressor import QuantizationAwareTrainingConfig\n","from neural_compressor.training import prepare_compression\n","\n","conf = QuantizationAwareTrainingConfig()\n","compression_manager = prepare_compression(model, conf)\n","compression_manager.callbacks.on_train_begin()\n","model = compression_manager.model\n","train_func(model)\n","compression_manager.callbacks.on_train_end()\n","compression_manager.save(\"./output\")\n","```\n","\n","\n","Chọn backend cho vào bên trong `conf`:\n","    \n","<table class=\"center\">\n","<thead>\n","    <tr>\n","        <th>Framework</th>\n","        <th>Backend</th>\n","        <th>Backend Library</th>\n","        <th>Backend Value</th>\n","        <th>Support Device</th>\n","    </tr>\n","</thead>\n","<tbody>\n","    <tr>\n","        <td rowspan=\"2\" align=\"left\">PyTorch</td>\n","        <td align=\"left\">FX</td>\n","        <td align=\"left\">FBGEMM</td>\n","        <td align=\"left\">\"default\"</td>\n","        <td align=\"left\">cpu</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">IPEX</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"ipex\"</td>\n","        <td align=\"left\">cpu | xpu</td>\n","    </tr>\n","    <tr>\n","        <td rowspan=\"5\" align=\"left\">ONNX Runtime</td>\n","        <td align=\"left\">CPUExecutionProvider</td>\n","        <td align=\"left\">MLAS</td>\n","        <td align=\"left\">\"default\"</td>\n","        <td align=\"left\">cpu</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">TensorrtExecutionProvider</td>\n","        <td align=\"left\">TensorRT</td>\n","        <td align=\"left\">\"onnxrt_trt_ep\"</td>\n","        <td align=\"left\">gpu</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">CUDAExecutionProvider</td>\n","        <td align=\"left\">CUDA</td>\n","        <td align=\"left\">\"onnxrt_cuda_ep\"</td>\n","        <td align=\"left\">gpu</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">DnnlExecutionProvider</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"onnxrt_dnnl_ep\"</td>\n","        <td align=\"left\">cpu</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">DmlExecutionProvider*</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"onnxrt_dml_ep\"</td>\n","        <td align=\"left\">npu</td>\n","    </tr>\n","    <tr>\n","        <td rowspan=\"2\" align=\"left\">Tensorflow</td>\n","        <td align=\"left\">Tensorflow</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"default\"</td>\n","        <td align=\"left\">cpu</td>\n","    </tr>\n","    <tr>\n","        <td align=\"left\">ITEX</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"itex\"</td>\n","        <td align=\"left\">cpu | gpu</td>\n","    </tr>  \n","    <tr>\n","        <td align=\"left\">MXNet</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">OneDNN</td>\n","        <td align=\"left\">\"default\"</td>\n","        <td align=\"left\">cpu</td>\n","    </tr>\n","</tbody>\n","</table>\n","<br>\n","<br>"],"metadata":{"id":"Dr0f6Lt34MF8"}}]}